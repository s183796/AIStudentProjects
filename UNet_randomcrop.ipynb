{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s183796/AIStudentProjects/blob/christine/UNet_randomcrop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8awFvthKz1k",
        "outputId": "090026d8-dd32-41f3-9220-6578c1fe6af7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from typing import *\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image, display, clear_output\n",
        "import numpy as np\n",
        "%matplotlib nbagg\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "from torch.nn.functional import relu\n",
        "from torch.nn.functional import softmax\n",
        "import PIL.Image\n",
        "import os\n",
        "import torchvision\n",
        "import cv2\n",
        "\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import glob\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "nmR7LP3jMgsN"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module): #from https://towardsdatascience.com/cook-your-first-u-net-in-pytorch-b3297a844cf3\n",
        "    def __init__(self, n_class):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder\n",
        "        # In the encoder, convolutional layers with the Conv2d function are used to extract features from the input image.\n",
        "        # Each block in the encoder consists of two convolutional layers followed by a max-pooling layer, with the exception of the last block which does not include a max-pooling layer.\n",
        "        # -------\n",
        "        # input: 1x128x128\n",
        "        self.e11 = nn.Conv2d(1, 64, kernel_size=3,padding=1)\n",
        "        self.e12 = nn.Conv2d(64, 64, kernel_size=3,padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) #64x64x64\n",
        "\n",
        "        self.e21 = nn.Conv2d(64, 128, kernel_size=3,padding=1)\n",
        "        self.e22 = nn.Conv2d(128, 128, kernel_size=3,padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) #32x32x128\n",
        "\n",
        "        self.e31 = nn.Conv2d(128, 256, kernel_size=3,padding=1)\n",
        "        self.e32 = nn.Conv2d(256, 256, kernel_size=3,padding=1)\n",
        "\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) #16x16x256\n",
        "\n",
        "        self.e41 = nn.Conv2d(256, 512, kernel_size=3,padding=1)\n",
        "        self.e42 = nn.Conv2d(512, 512, kernel_size=3,padding=1)\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2) #8x8x512\n",
        "\n",
        "        self.e51 = nn.Conv2d(512, 1024, kernel_size=3,padding=1)\n",
        "        self.e52 = nn.Conv2d(1024, 1024, kernel_size=3,padding=1)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(1024,512,kernel_size=2,stride=2) #16x16x1024\n",
        "        self.d11 = nn.Conv2d(1024,512,kernel_size=3,padding=1)\n",
        "        self.d12 = nn.Conv2d(512,512,kernel_size=3,padding=1)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(512,256,kernel_size=2,stride=2) #\n",
        "        self.d21 = nn.Conv2d(512,256,kernel_size=3,padding=1)\n",
        "        self.d22 = nn.Conv2d(256,256,kernel_size=3,padding=1)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(256,128,kernel_size=2,stride=2)\n",
        "        self.d31 = nn.Conv2d(256,128,kernel_size=3,padding=1)\n",
        "        self.d32 = nn.Conv2d(128,128,kernel_size=3,padding=1)\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(128,64,kernel_size=2,stride=2)\n",
        "        self.d41 = nn.Conv2d(128,64,kernel_size=3,padding=1)\n",
        "        self.d42 = nn.Conv2d(64,64,kernel_size=3,padding=1)\n",
        "\n",
        "        self.outconv = nn.Conv2d(64, n_class, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        xe11 = F.relu(self.e11(x))\n",
        "        xe12 = F.relu(self.e12(xe11))\n",
        "        xp1 = self.pool1(xe12)\n",
        "\n",
        "        xe21 = F.relu(self.e21(xp1))\n",
        "        xe22 = F.relu(self.e22(xe21))\n",
        "        xp2 = self.pool2(xe22)\n",
        "\n",
        "\n",
        "        xe31 = F.relu(self.e31(xp2))\n",
        "        xe32 = F.relu(self.e32(xe31))\n",
        "\n",
        "\n",
        "        xp3 = self.pool3(xe32)\n",
        "\n",
        "        xe41 = F.relu(self.e41(xp3))\n",
        "        xe42 = F.relu(self.e42(xe41))\n",
        "        xp4 = self.pool4(xe42)\n",
        "\n",
        "        xe51 = F.relu(self.e51(xp4))\n",
        "        xe52 = F.relu(self.e52(xe51))\n",
        "\n",
        "        # Up-convolutions\n",
        "        xup1 = self.upconv1(xe52)\n",
        "\n",
        "        xcat = torch.cat([xup1, xe42], dim=1)\n",
        "\n",
        "        xup21 = F.relu(self.d11(xcat))\n",
        "        xup22 = F.relu(self.d12(xup21))\n",
        "\n",
        "        xup2 = self.upconv2(xup22)\n",
        "\n",
        "        #xcat2 = torch.cat([xup2, xe32[:,:,:-1,:-1]], dim=1)\n",
        "        xcat2 = torch.cat([xup2, xe32], dim=1)\n",
        "\n",
        "\n",
        "        xup31 = F.relu(self.d21(xcat2))\n",
        "        xup32 = F.relu(self.d22(xup31))\n",
        "        xup3 = self.upconv3(xup32)\n",
        "        xcat3 = torch.cat([xup3, xe22], dim=1)\n",
        "\n",
        "        xup41 = F.relu(self.d31(xcat3))\n",
        "        xup42 = F.relu(self.d32(xup41))\n",
        "\n",
        "        xup4 = self.upconv4(xup42)\n",
        "        #xcat4 = torch.cat([xup4, xe12[:,:,2:-3,2:-3]], dim=1)\n",
        "        xcat4 = torch.cat([xup4, xe12], dim=1)\n",
        "        xup51 = F.relu(self.d41(xcat4))\n",
        "        xup52 = F.relu(self.d42(xup51))\n",
        "\n",
        "        out = self.outconv(xup52)\n",
        "\n",
        "        output = softmax(out,dim=1)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Qzsg0ha3ESMZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ax5Mtst7Eoeo"
      },
      "outputs": [],
      "source": [
        "\n",
        "net=UNet(n_class=3) #running model\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5), (0.5)),  #  mean=0.5 and sigma=0.5\n",
        "    ]\n",
        ")\n",
        "\n",
        "#image=cv2.imread('SOCprist0001.tiff',cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "#print(net(transform(image[0:128,0:128])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "RT2I5QamDF2X"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "smbMc8I45k87"
      },
      "outputs": [],
      "source": [
        "#Setting up hyper parameters, from exercise week 6\n",
        "\n",
        "LEARNING_RATE = 0.001\n",
        "loss_fn =  nn.CrossEntropyLoss()         # <-- Your code here.\n",
        "#loss_fn =  nn.BCEWithLogitsLoss()         # <-- Your code here.\n",
        "\n",
        "def accuracy(ys, ts):\n",
        "    correct_prediction = torch.eq(ys, ts)\n",
        "    return torch.mean(correct_prediction.float())\n",
        "\n",
        "def UnetLoss(preds, targets): #from https://wandb.ai/ishandutta/semantic_segmentation_unet/reports/Semantic-Segmentation-with-UNets-in-PyTorch--VmlldzoyMzA3MTk1\n",
        "    ce_loss = ce(preds, targets)\n",
        "    acc = (torch.max(preds, 1)[1] == targets).float().mean()\n",
        "    return ce_loss, acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "sqCSVV6cE9Un"
      },
      "outputs": [],
      "source": [
        "class SOCDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.image_folder = os.path.join(root_dir, 'data/')\n",
        "        self.label_folder = os.path.join(root_dir, 'labels/')\n",
        "        self.transform = transform\n",
        "\n",
        "        self.image_filenames = [f for f in os.listdir(self.image_folder) if f.endswith('.tiff')][:200]\n",
        "        self.label_filenames = [f for f in os.listdir(self.label_folder) if f.endswith('.tif')][:200]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      img_name = os.path.join(self.image_folder, self.image_filenames[idx])\n",
        "      label_name = os.path.join(self.label_folder, self.label_filenames[idx])\n",
        "\n",
        "      image = cv2.imread(img_name, cv2.IMREAD_GRAYSCALE)\n",
        "      label = cv2.imread(label_name, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "      if self.transform:\n",
        "          image = self.transform(image)\n",
        "\n",
        "      i, j, h, w = transforms.RandomCrop.get_params(torch.zeros_like(image),output_size=(128,128))\n",
        "      image = image[:,i:i+h, j:j+w]\n",
        "\n",
        "\n",
        "\n",
        "      label=label[i:i+h, j:j+w]\n",
        "\n",
        "\n",
        "      return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "oqDLd6aV1MIB"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5), (0.5)),  # subtract 0.5 and divide by 0.5\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "UpKsXg_j1Gy1"
      },
      "outputs": [],
      "source": [
        "SOC_dataset = SOCDataset(root_dir='drive/My Drive//AI data/', transform = transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEKJRtDj2Omv",
        "outputId": "44f720ee-7d0c-4647-986d-aa9860e281f1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu118)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.10.0)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (23.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ZlW8ER1KGNL8"
      },
      "outputs": [],
      "source": [
        "from torchmetrics.classification import JaccardIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "97zj0_jLGX1n"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "gx_50U0KGbSB"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "TEST_SIZE = 0.1\n",
        "BATCH_SIZE = 24\n",
        "SEED = 42\n",
        "\n",
        "# generate indices: instead of the actual data we pass in integers instead\n",
        "train_indices, test_indices = train_test_split(\n",
        "    range(len(SOC_dataset)),\n",
        "    test_size=TEST_SIZE,\n",
        "    random_state=SEED\n",
        ")\n",
        "\n",
        "# generate subset based on indices\n",
        "train_split = Subset(SOC_dataset, train_indices)\n",
        "test_split = Subset(SOC_dataset, test_indices)\n",
        "\n",
        "# create batches\n",
        "train_loader = DataLoader(train_split, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_split, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "hBjIRQ-eBTNO",
        "outputId": "a5d1b0c2-b12a-4a22-8984-cdd39fef7b04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 10      training accuracy: 0.09271104633808136\n",
            "             test accuracy: 0.11238868534564972\n",
            "Step 20      training accuracy: 0.0975392609834671\n",
            "             test accuracy: 0.10518430918455124\n",
            "Step 30      training accuracy: 0.10329124331474304\n",
            "             test accuracy: 0.11631761491298676\n",
            "Step 40      training accuracy: 0.10556330531835556\n",
            "             test accuracy: 0.1185183897614479\n",
            "Step 50      training accuracy: 0.1448354721069336\n",
            "             test accuracy: 0.13534437119960785\n",
            "Step 60      training accuracy: 0.13494618237018585\n",
            "             test accuracy: 0.1458892822265625\n",
            "Step 70      training accuracy: 0.14425913989543915\n",
            "             test accuracy: 0.13390196859836578\n",
            "Step 80      training accuracy: 0.13921090960502625\n",
            "             test accuracy: 0.14378255605697632\n",
            "Step 90      training accuracy: 0.14176517724990845\n",
            "             test accuracy: 0.13844400644302368\n",
            "Step 100     training accuracy: 0.13762536644935608\n",
            "             test accuracy: 0.13244323432445526\n",
            "Step 110     training accuracy: 0.14120398461818695\n",
            "             test accuracy: 0.14312744140625\n",
            "Step 120     training accuracy: 0.14585526287555695\n",
            "             test accuracy: 0.13525797426700592\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-4aac0452a6dc>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# Compute gradients based on the loss from the current batch (backpropagation).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Take one optimizer step using the gradients computed in the previous step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "net=UNet(n_class=3)\n",
        "\n",
        "metric=JaccardIndex(task=\"multiclass\", num_classes=3)\n",
        "\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001,momentum=0.95)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "net.to(device)\n",
        "\n",
        "batch_size = 12\n",
        "num_epochs = 20 #changing the num_epochs from 2 to 12\n",
        "validation_every_steps = 10\n",
        "\n",
        "step = 0\n",
        "net.train()\n",
        "\n",
        "train_accuracies = []\n",
        "valid_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    train_accuracies_batches = []\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Forward pass, compute gradients, perform one training step.\n",
        "        # Your code here!\n",
        "        # Forward pass.\n",
        "        output = net(inputs)\n",
        "\n",
        "        # Compute loss.\n",
        "        targets[targets==0]=0\n",
        "        targets[targets==128]=1\n",
        "        targets[targets==255]=2\n",
        "\n",
        "        targets = targets.to(torch.int64)\n",
        "\n",
        "        loss = loss_fn(output, targets)\n",
        "\n",
        "        # Clean up gradients from the model.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Compute gradients based on the loss from the current batch (backpropagation).\n",
        "        loss.backward()\n",
        "\n",
        "        # Take one optimizer step using the gradients computed in the previous step.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Increment step counter\n",
        "        step += 1\n",
        "\n",
        "        # Compute accuracy.\n",
        "        predictions = torch.argmax(output,dim=1)\n",
        "        train_accuracies_batches.append(metric(predictions,targets))\n",
        "\n",
        "        if step % validation_every_steps == 0:\n",
        "\n",
        "            # Append average training accuracy to list.\n",
        "            train_accuracies.append(np.mean(train_accuracies_batches))\n",
        "\n",
        "            train_accuracies_batches = []\n",
        "\n",
        "            # Compute accuracies on validation set.\n",
        "            valid_accuracies_batches = []\n",
        "            with torch.no_grad():\n",
        "                net.eval()\n",
        "                for inputs, targets in test_loader:\n",
        "                    inputs, targets = inputs.to(device), targets.to(device)\n",
        "                    output = net(inputs)\n",
        "\n",
        "\n",
        "                    targets[targets==0]=0\n",
        "                    targets[targets==128]=1\n",
        "                    targets[targets==255]=2\n",
        "\n",
        "                    targets = targets.to(torch.int64)\n",
        "\n",
        "                    loss = loss_fn(output, targets)\n",
        "\n",
        "                    predictions = output.max(1)[1]\n",
        "                    # Multiply by len(x) because the final batch of DataLoader may be smaller (drop_last=False).\n",
        "                    valid_accuracies_batches.append(metric(predictions, targets))\n",
        "\n",
        "                net.train()\n",
        "\n",
        "            # Append average validation accuracy to list.\n",
        "            valid_accuracies.append(np.sum(valid_accuracies_batches) / len(test_loader))\n",
        "\n",
        "            print(f\"Step {step:<5}   training accuracy: {train_accuracies[-1]}\")\n",
        "            print(f\"             test accuracy: {valid_accuracies[-1]}\")\n",
        "\n",
        "print(\"Finished training.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vP3jaiMblzqL"
      },
      "outputs": [],
      "source": [
        "targets.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TY0r8tPEQvAU"
      },
      "outputs": [],
      "source": [
        "fig,axs = plt.subplots(1,3,figsize=(15,15))\n",
        "\n",
        "axs[0].imshow(output.detach().numpy()[0,0,:,:])\n",
        "axs[0].set_title('Label 1')\n",
        "axs[1].imshow(output.detach().numpy()[0,1,:,:])\n",
        "axs[1].set_title('Label 2')\n",
        "axs[2].imshow(output.detach().numpy()[0,2,:,:])\n",
        "axs[2].set_title('Label 3')\n",
        "\n",
        "# Create a common colorbar\n",
        "#cax = fig.add_axes([0.93, 0.1, 0.02, 0.8])  # Adjust the position and size of the colorbar\n",
        "\n",
        "# You can use any of the axes to create the colorbar; here, I've used axs[2]\n",
        "\n",
        "\n",
        "# Set the label for the colorbar\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5S23duPOtqvm"
      },
      "outputs": [],
      "source": [
        "np.max(output.detach().numpy()[0,2,:,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oIDqfAO1kl2"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1,3,figsize=(15,15))\n",
        "axs[0].imshow(im_true[0,:,:].detach().numpy())\n",
        "axs[0].set_title('Label 1')\n",
        "axs[1].imshow(im_true[1,:,:].detach().numpy())\n",
        "axs[1].set_title('Label 2')\n",
        "axs[2].imshow(im_true[2,:,:].detach().numpy())\n",
        "axs[2].set_title('Label 3')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8AXGLJe5GfJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UKsO0WmOYEi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6YVamuFOTtS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9+zy4ocLmdmzJCF8gwt2e",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}